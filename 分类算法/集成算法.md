# 集成学习（Ensemble Learn）

## 学习器分类

1. 强学习器：Strong learner

    预测结果较好的学习器

2. 弱学习器：Weak learner

    预测结果较弱的学习器

3. 基学习器：Base learner

    集成学习中的个体学习器，一般是弱学习器

4. 基学习算法：Base learner algorithm

    基学习器所使用的方法

5. 同质学习器：Homogeneous Base Learner

    集成的学习器的基学习算法一样

6. 异质学习器：Heterogeneous Base Learner

    集成的学习器的基学习算法不一致

## 集成学习算法

第一步 构建基学习器

第二部 组合基学习器

两种构造方式

1. 平行方式

    - 基学习器之间不存在强依赖
    - 代表算法：Bagging和随机森林算法

2. 顺序化方法

    - 多个学习器是依次构建的
    - 之间有强依赖关系
    - 代表算法: Boosting


## 三类算法详解（Boosting\Bagging\Stacking）

### Boosting

    训练过程为阶梯状，弱分类器按次序一一进行训练（实现上可以做到并行），弱分类器的训练集按照某种策略每次都进行一定的转化。最后以一定的方式将弱分类器组合成一个强分类器。

- AdaBoost算法

    将弱分类器联合起来，使用加权的投票机制代替平均投票机制。让分类效果好的分类器具有较大的权重。

    有两个权值：样本权值/弱分类器权值

    - 样本权值：分类错误样本，加大权值
    - 弱分类器权值： 越准确权值越大

    优缺点：
    
    - 优点：精度高，可解释，不易发生过拟合
    - 缺点：对异常样本敏感

    算法过程

    1. 给定训练样本集S，其中X和Y分别为正例样本和负例样本；T为训练的最大循环次数
    2. 初始化样本的权重为1/n，即为训练样本的初试概率分布
    3. 第一次迭代：
        - 训练样本的分布相当下，训练弱分类器
        - 计算弱分类器的错误率
        - 选取合适的阈值，使得误差最小
        - 更新样本权重
    4. 经过T次迭代，得到T个弱分类器，按更新的弱分类器权重叠加，最终得到强分类器

- Gradient Boosting算法

    核心思想：每个新的弱分类器的建立是为了之前的分类器的残差往梯度方向减少，然后把弱分类器进行累加得到强分类器
    
    所有弱分类器只能是CART回归树

    优缺点：

    - 优点：预测精度高，对异常值的鲁棒性
    - 缺点：难以并行训练

    算法过程：
    1. 对数据拟合一个简单的线性回归或决策树
    2. 计算误差残差，实际目标值减去预测目标值
    3. 将误差残差的新模型作为具有相同输入变量和目标变量
    4. 将预测的残差添加到先前的预测中[y_predicted2 = y_predicted1 + e1_predicted]
    5. 在剩余的残差上拟合另一个模型。即[e2 = y-y_predicted2]并重复步骤2到5，直到它开始过拟合或残差总和变成恒定。

### Bagging

    核心思想：是统计学中的一种抽样方法。利用有限的样本，经由多次重复抽样（从给定训练集中有放回的均匀抽样，也就是说，每当选中一个样本，其等可能的被再次选中并被再次添加回训练集中），建立起足以代表母体样本分布的新样本。

    平行算法

算法过程：

    1. 首先在bootstrap抽样的数据集上，通过调用基学习算法，训练出一系列的基学习器
    2. 通过多数投票，最多票数的类别被选为结果

### Stacking

算法过程：

    1. 通过调用基学习器，生成若干独立的基学习器（一级学习器）
    2. 接着这些独立的基学习器被通过元学习器（二级学习器）组合起来，其方式是将一级学习器的输出作为输入，进行训练。

    