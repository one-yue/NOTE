# 决策树

## 1. 决策树工作原理

构建一棵内部节点和根节点都是判断条件，子节点是结果的一颗树

## 2. 建立决策树

### 1. Hunt算法

设$D_t$是与结点t相关联的训练记录及，而$y=\{y_1,y_2,\dots,y_c\}$是类标号（类标号=结果）。

算法过程：

1. 如果$D_t$中所有记录都属于同一类$y_t$，则$t$是叶子结点，用$y_t$标记。

2. 如果$D_t$始终抱恨属于多个类的记录，则选择一个属性测试条件，将记录划分成较小的自己。对于测试条件的每个输出，创建一个子女结点，并根据测试结果将$D_t$中的记录分布到子女结点中。然后，对于每个子女结点，，递归地钓友该算法。

算法的两个问题：

1. 如何分裂训练记录：不同的分裂方式对应不同的决策树算法：ID3,C4.5,CART

2. 如何停止分裂过程：如果没有合适的停止时间，会导致不拟合或者过拟合。

### 2. 选择最佳划分的度量

设$p(i|t)$表示给定结点$t$中属于类$i$的记录所占的比例

熵：$Entropy(t)=-\sum_{i = 0}^{c-1}p(i|t)\log_2p)(i|t)$，在ID3和C4.5算法中使用。

Gini：$Gini(t)=1-\sum_{i = 0}^{c-1}[p(i|t)]^2$，在CART算法中使用

熵和Gini都是用来表示不纯度，不纯度越小，类分布就越倾斜。

增益$\Delta$：比较父结点（划分前）的不纯度和子女结点（划分后）的不纯度。差值越大，测试跳进的效果就越好。
$$
\Delta=I(parent)-\sum_{j=1}^k \frac{N(v_j)}{N}I(v_j)
$$
$I(parent)$是一个不变的值，所以最大化增益等价为子女结点的最小化增益。熵差称为信息增益($\Delta_{info}$)。